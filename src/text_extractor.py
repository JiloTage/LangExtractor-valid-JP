"""
LangExtractã‚’ä½¿ç”¨ã—ã¦æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
"""
from typing import List, Dict, Optional
from dataclasses import dataclass, asdict
from enum import Enum
import re
import time
import langextract as lx
from dotenv import load_dotenv

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()


class Gender(Enum):
    """æ€§åˆ¥ã®åˆ—æŒ™å‹"""
    MALE = "ç”·æ€§"
    FEMALE = "å¥³æ€§"
    UNKNOWN = "ä¸æ˜"


@dataclass
class Character:
    """ç™»å ´äººç‰©ã®ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    name: str
    gender: str
    age: Optional[str]
    occupation: Optional[str]
    appearance: Optional[str]
    personality: Optional[str]
    
    def to_dict(self):
        return asdict(self)


@dataclass
class Emotion:
    """æ„Ÿæƒ…ã®ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    emotion_type: str
    subject: str
    target: Optional[str]
    intensity: str
    quote: str
    
    def to_dict(self):
        return asdict(self)


@dataclass
class Relationship:
    """é–¢ä¿‚æ€§ã®ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    person1: str
    person2: str
    relation_type: str
    direction: str
    evidence: str
    
    def to_dict(self):
        return asdict(self)


class JapaneseTextExtractor:
    """æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆç”¨ã®LangExtractå®Ÿè¡Œã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, model_id: str = None):
        """
        åˆæœŸåŒ–
        Args:
            model_id: ä½¿ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«ï¼ˆNoneã®å ´åˆã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
        """
        import os
        
        # ãƒ¢ãƒ‡ãƒ«IDã®æ±ºå®šï¼ˆå„ªå…ˆé †ä½: å¼•æ•° > ç’°å¢ƒå¤‰æ•° > ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
        if model_id is None:
            model_id = os.getenv("LANGEXTRACT_MODEL", "gemini-2.0-flash-exp")
        
        self.model_id = model_id
        
        # APIã‚­ãƒ¼ã®å–å¾—ï¼ˆãƒ¢ãƒ‡ãƒ«ã«å¿œã˜ã¦é©åˆ‡ãªAPIã‚­ãƒ¼ã‚’é¸æŠï¼‰
        if "gpt" in model_id.lower():
            self.api_key = os.getenv("OPENAI_API_KEY")
            if not self.api_key:
                raise ValueError("OpenAIãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯OPENAI_API_KEYãŒå¿…è¦ã§ã™")
        else:
            # Geminiã‚„ãã®ä»–ã®ãƒ¢ãƒ‡ãƒ«
            self.api_key = os.getenv("GOOGLE_API_KEY")
            if not self.api_key:
                raise ValueError("Geminiãƒ¢ãƒ‡ãƒ«ã¾ãŸã¯Googleç³»ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯GOOGLE_API_KEYãŒå¿…è¦ã§ã™")
        
        print(f"ğŸ¤– ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {self.model_id}")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        self.prompts = {
            "character": self._get_character_prompt(),
            "emotion": self._get_emotion_prompt(),
            "relationship": self._get_relationship_prompt()
        }
        
        # æŠ½å‡ºä¾‹
        self.examples = {
            "character": self._get_character_examples(),
            "emotion": self._get_emotion_examples(),
            "relationship": self._get_relationship_examples()
        }
    
    def extract_all(self, text: str, max_chunk_size: int = 3000, use_scaling: bool = True, 
                   extraction_passes: int = 2, max_workers: int = 10, max_char_buffer: int = 1000) -> Dict[str, List]:
        """
        ã™ã¹ã¦ã®æƒ…å ±ã‚’æŠ½å‡ºï¼ˆå¤§å®¹é‡ãƒ†ã‚­ã‚¹ãƒˆå¯¾å¿œãƒ»LangExtractã®Scaling to Longer Documentsæ©Ÿèƒ½ä½¿ç”¨ï¼‰
        Args:
            text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
            max_chunk_size: ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã®æœ€å¤§ã‚µã‚¤ã‚º
            use_scaling: LangExtractã®scalingè¨­å®šã‚’ä½¿ç”¨ã™ã‚‹ã‹
            extraction_passes: æŠ½å‡ºãƒ‘ã‚¹æ•°ï¼ˆè¤‡æ•°å›å®Ÿè¡Œã§ç²¾åº¦å‘ä¸Šï¼‰
            max_workers: ä¸¦åˆ—å‡¦ç†ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
            max_char_buffer: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º
        Returns:
            æŠ½å‡ºçµæœã®è¾æ›¸
        """
        print("ğŸ” æƒ…å ±æŠ½å‡ºã‚’é–‹å§‹ã—ã¾ã™...")
        
        # LangExtractã®Scalingæ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ
        if use_scaling and len(text) > max_chunk_size:
            print(f"ğŸš€ LangExtractã®Scalingæ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¦é•·æ–‡å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™")
            print(f"   æŠ½å‡ºãƒ‘ã‚¹æ•°: {extraction_passes}, ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼: {max_workers}, ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º: {max_char_buffer}")
            
            try:
                # Scalingæ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ãŸæŠ½å‡º
                results = {
                    "characters": self._extract_with_scaling(text, "character", extraction_passes, max_workers, max_char_buffer),
                    "emotions": self._extract_with_scaling(text, "emotion", extraction_passes, max_workers, max_char_buffer),
                    "relationships": self._extract_with_scaling(text, "relationship", extraction_passes, max_workers, max_char_buffer)
                }
                
                print("âœ… Scalingæ©Ÿèƒ½ã«ã‚ˆã‚‹æŠ½å‡ºå®Œäº†!")
                return results
                
            except Exception as e:
                print(f"âš ï¸ Scalingæ©Ÿèƒ½ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
                print("   ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã«åˆ‡ã‚Šæ›¿ãˆã¾ã™...")
        
        # ãƒ†ã‚­ã‚¹ãƒˆãŒé•·ã„å ´åˆã¯ãƒãƒ£ãƒ³ã‚¯åŒ–ï¼ˆå¾“æ¥ã®æ–¹æ³•ï¼‰
        if len(text) > max_chunk_size:
            print(f"ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆãŒé•·ã„ãŸã‚ã€{max_chunk_size}æ–‡å­—ãšã¤ã«åˆ†å‰²ã—ã¦å‡¦ç†ã—ã¾ã™")
            chunks = self._chunk_text(text, max_chunk_size)
            print(f"ğŸ“ {len(chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¾ã—ãŸ")
            
            # å„ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰æŠ½å‡ºã—ã¦ãƒãƒ¼ã‚¸
            all_characters = []
            all_emotions = []
            all_relationships = []
            
            for i, chunk in enumerate(chunks, 1):
                print(f"  ğŸ”„ ãƒãƒ£ãƒ³ã‚¯ {i}/{len(chunks)} ã‚’å‡¦ç†ä¸­...")
                try:
                    chunk_results = {
                        "characters": self._safe_extract_characters(chunk),
                        "emotions": self._safe_extract_emotions(chunk),
                        "relationships": self._safe_extract_relationships(chunk)
                    }
                    
                    all_characters.extend(chunk_results["characters"])
                    all_emotions.extend(chunk_results["emotions"])
                    all_relationships.extend(chunk_results["relationships"])
                    
                    # ãƒãƒ£ãƒ³ã‚¯é–“ã®å¾…æ©Ÿæ™‚é–“ï¼ˆAPIåˆ¶é™å¯¾ç­–ï¼‰
                    if i < len(chunks):
                        print(f"    â³ APIåˆ¶é™å›é¿ã®ãŸã‚2ç§’å¾…æ©Ÿä¸­...")
                        time.sleep(2)
                    
                except Exception as e:
                    print(f"    âš ï¸ ãƒãƒ£ãƒ³ã‚¯{i}ã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
                    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯é•·ã‚ã«å¾…æ©Ÿ
                    if "429" in str(e) or "RESOURCE_EXHAUSTED" in str(e):
                        print(f"    â±ï¸ ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼: 60ç§’å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
                        time.sleep(60)
                        try:
                            # ãƒªãƒˆãƒ©ã‚¤
                            chunk_results = {
                                "characters": self._safe_extract_characters(chunk),
                                "emotions": self._safe_extract_emotions(chunk), 
                                "relationships": self._safe_extract_relationships(chunk)
                            }
                            all_characters.extend(chunk_results["characters"])
                            all_emotions.extend(chunk_results["emotions"])
                            all_relationships.extend(chunk_results["relationships"])
                            print(f"    âœ… ãƒªãƒˆãƒ©ã‚¤æˆåŠŸ")
                        except Exception as retry_e:
                            print(f"    âŒ ãƒªãƒˆãƒ©ã‚¤ã‚‚å¤±æ•—: {retry_e}")
                    continue
            
            # é‡è¤‡é™¤å»
            results = {
                "characters": self._deduplicate_characters(all_characters),
                "emotions": all_emotions,  # æ„Ÿæƒ…ã¯é‡è¤‡ãŒã‚ã£ã¦ã‚‚å•é¡Œãªã„
                "relationships": self._deduplicate_relationships(all_relationships)
            }
        else:
            # é€šå¸¸å‡¦ç†
            results = {
                "characters": self._safe_extract_characters(text),
                "emotions": self._safe_extract_emotions(text),
                "relationships": self._safe_extract_relationships(text)
            }
        
        print("âœ… æŠ½å‡ºå®Œäº†!")
        return results
    
    def extract_characters(self, text: str) -> List[Character]:
        """ç™»å ´äººç‰©ã‚’æŠ½å‡º"""
        print("  ğŸ‘¤ ç™»å ´äººç‰©ã‚’æŠ½å‡ºä¸­...")
        
        try:
            result = lx.extract(
                text_or_documents=text,
                prompt_description=self.prompts["character"],
                examples=self.examples["character"],
                model_id=self.model_id,
                api_key=self.api_key
            )
            
            # çµæœã‚’Characterã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
            characters = []
            for extraction in result.extractions:
                attrs = extraction.attributes
                char = Character(
                    name=extraction.extraction_text or attrs.get("name", "ä¸æ˜"),
                    gender=attrs.get("gender", "ä¸æ˜"),
                    age=attrs.get("age"),
                    occupation=attrs.get("occupation"),
                    appearance=attrs.get("appearance"),
                    personality=attrs.get("personality")
                )
                characters.append(char)
            
            print(f"    â†’ {len(characters)}äººã®ç™»å ´äººç‰©ã‚’ç™ºè¦‹")
            return characters
            
        except Exception as e:
            print(f"    âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def extract_emotions(self, text: str) -> List[Emotion]:
        """æ„Ÿæƒ…ã‚’æŠ½å‡º"""
        print("  ğŸ’­ æ„Ÿæƒ…ã‚’æŠ½å‡ºä¸­...")
        
        try:
            result = lx.extract(
                text_or_documents=text,
                prompt_description=self.prompts["emotion"],
                examples=self.examples["emotion"],
                model_id=self.model_id,
                api_key=self.api_key
            )
            
            emotions = []
            for extraction in result.extractions:
                attrs = extraction.attributes
                emotion = Emotion(
                    emotion_type=attrs.get("emotion_type", "ä¸æ˜"),
                    subject=attrs.get("subject", "ä¸æ˜"),
                    target=attrs.get("target"),
                    intensity=attrs.get("intensity", "æ™®é€š"),
                    quote=extraction.extraction_text or attrs.get("quote", "")
                )
                emotions.append(emotion)
            
            print(f"    â†’ {len(emotions)}å€‹ã®æ„Ÿæƒ…ã‚’ç™ºè¦‹")
            return emotions
            
        except Exception as e:
            print(f"    âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def extract_relationships(self, text: str) -> List[Relationship]:
        """é–¢ä¿‚æ€§ã‚’æŠ½å‡º"""
        print("  ğŸ”— é–¢ä¿‚æ€§ã‚’æŠ½å‡ºä¸­...")
        
        try:
            result = lx.extract(
                text_or_documents=text,
                prompt_description=self.prompts["relationship"],
                examples=self.examples["relationship"],
                model_id=self.model_id,
                api_key=self.api_key
            )
            
            relationships = []
            for extraction in result.extractions:
                attrs = extraction.attributes
                rel = Relationship(
                    person1=attrs.get("person1", "ä¸æ˜"),
                    person2=attrs.get("person2", "ä¸æ˜"),
                    relation_type=attrs.get("relation_type", "ä¸æ˜"),
                    direction=attrs.get("direction", "ä¸€æ–¹å‘"),
                    evidence=extraction.extraction_text or attrs.get("evidence", "")
                )
                relationships.append(rel)
            
            print(f"    â†’ {len(relationships)}å€‹ã®é–¢ä¿‚æ€§ã‚’ç™ºè¦‹")
            return relationships
            
        except Exception as e:
            print(f"    âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _chunk_text(self, text: str, max_size: int) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’é©åˆ‡ãªã‚µã‚¤ã‚ºã«ãƒãƒ£ãƒ³ã‚¯åŒ–"""
        chunks = []
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', text)
        
        current_chunk = ""
        for sentence in sentences:
            if len(current_chunk) + len(sentence) <= max_size:
                current_chunk += sentence + "ã€‚"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + "ã€‚"
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def _safe_extract_characters(self, text: str) -> List[Character]:
        """å®‰å…¨ãªç™»å ´äººç‰©æŠ½å‡ºï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰"""
        try:
            import json
            result = lx.extract(
                text_or_documents=text,
                prompt_description=self.prompts["character"],
                examples=self.examples["character"],
                model_id=self.model_id,
                api_key=self.api_key
            )
            
            characters = []
            for extraction in result.extractions:
                attrs = extraction.attributes
                char = Character(
                    name=extraction.extraction_text or attrs.get("name", "ä¸æ˜"),
                    gender=attrs.get("gender", "ä¸æ˜"),
                    age=attrs.get("age"),
                    occupation=attrs.get("occupation"),
                    appearance=attrs.get("appearance"),
                    personality=attrs.get("personality")
                )
                characters.append(char)
            
            return characters
            
        except json.JSONDecodeError as e:
            print(f"    âš ï¸ JSONè§£æã‚¨ãƒ©ãƒ¼: ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚ºã‚’ç¸®å°ã—ã¦ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™")
            # ã‚ˆã‚Šå°ã•ãªãƒãƒ£ãƒ³ã‚¯ã§å†è©¦è¡Œ
            if len(text) > 1000:
                smaller_chunks = self._chunk_text(text, 1000)
                all_chars = []
                for chunk in smaller_chunks[:3]:  # æœ€åˆã®3ãƒãƒ£ãƒ³ã‚¯ã®ã¿
                    try:
                        chunk_chars = self._safe_extract_characters(chunk)
                        all_chars.extend(chunk_chars)
                    except:
                        continue
                return all_chars
            return []
        except Exception as e:
            print(f"    âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _safe_extract_emotions(self, text: str) -> List[Emotion]:
        """å®‰å…¨ãªæ„Ÿæƒ…æŠ½å‡ºï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰"""
        try:
            import json
            result = lx.extract(
                text_or_documents=text,
                prompt_description=self.prompts["emotion"],
                examples=self.examples["emotion"],
                model_id=self.model_id,
                api_key=self.api_key
            )
            
            emotions = []
            for extraction in result.extractions:
                attrs = extraction.attributes
                emotion = Emotion(
                    emotion_type=attrs.get("emotion_type", "ä¸æ˜"),
                    subject=attrs.get("subject", "ä¸æ˜"),
                    target=attrs.get("target"),
                    intensity=attrs.get("intensity", "æ™®é€š"),
                    quote=extraction.extraction_text or attrs.get("quote", "")
                )
                emotions.append(emotion)
            
            return emotions
            
        except json.JSONDecodeError as e:
            print(f"    âš ï¸ JSONè§£æã‚¨ãƒ©ãƒ¼: ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚ºã‚’ç¸®å°ã—ã¦ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™")
            if len(text) > 1000:
                smaller_chunks = self._chunk_text(text, 1000)
                all_emotions = []
                for chunk in smaller_chunks[:3]:
                    try:
                        chunk_emotions = self._safe_extract_emotions(chunk)
                        all_emotions.extend(chunk_emotions)
                    except:
                        continue
                return all_emotions
            return []
        except Exception as e:
            print(f"    âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _safe_extract_relationships(self, text: str) -> List[Relationship]:
        """å®‰å…¨ãªé–¢ä¿‚æ€§æŠ½å‡ºï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰"""
        try:
            import json
            result = lx.extract(
                text_or_documents=text,
                prompt_description=self.prompts["relationship"],
                examples=self.examples["relationship"],
                model_id=self.model_id,
                api_key=self.api_key
            )
            
            relationships = []
            for extraction in result.extractions:
                attrs = extraction.attributes
                rel = Relationship(
                    person1=attrs.get("person1", "ä¸æ˜"),
                    person2=attrs.get("person2", "ä¸æ˜"),
                    relation_type=attrs.get("relation_type", "ä¸æ˜"),
                    direction=attrs.get("direction", "ä¸€æ–¹å‘"),
                    evidence=extraction.extraction_text or attrs.get("evidence", "")
                )
                relationships.append(rel)
            
            return relationships
            
        except json.JSONDecodeError as e:
            print(f"    âš ï¸ JSONè§£æã‚¨ãƒ©ãƒ¼: ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚ºã‚’ç¸®å°ã—ã¦ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™")
            if len(text) > 1000:
                smaller_chunks = self._chunk_text(text, 1000)
                all_rels = []
                for chunk in smaller_chunks[:3]:
                    try:
                        chunk_rels = self._safe_extract_relationships(chunk)
                        all_rels.extend(chunk_rels)
                    except:
                        continue
                return all_rels
            return []
        except Exception as e:
            print(f"    âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _deduplicate_characters(self, characters: List[Character]) -> List[Character]:
        """ç™»å ´äººç‰©ã®é‡è¤‡é™¤å»"""
        seen_names = set()
        unique_chars = []
        
        for char in characters:
            # åå‰ã®æ­£è¦åŒ–ï¼ˆç©ºç™½é™¤å»ã€å°æ–‡å­—åŒ–ï¼‰
            normalized_name = char.name.replace(" ", "").replace("ã€€", "").lower()
            if normalized_name not in seen_names:
                seen_names.add(normalized_name)
                unique_chars.append(char)
        
        return unique_chars
    
    def _deduplicate_relationships(self, relationships: List[Relationship]) -> List[Relationship]:
        """é–¢ä¿‚æ€§ã®é‡è¤‡é™¤å»"""
        seen_rels = set()
        unique_rels = []
        
        for rel in relationships:
            # é–¢ä¿‚æ€§ã®æ­£è¦åŒ–
            rel_key = f"{rel.person1}_{rel.person2}_{rel.relation_type}"
            if rel_key not in seen_rels:
                seen_rels.add(rel_key)
                unique_rels.append(rel)
        
        return unique_rels
    
    def _extract_with_scaling(self, text: str, extraction_type: str, extraction_passes: int, 
                             max_workers: int, max_char_buffer: int) -> List:
        """
        LangExtractã®Scalingæ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ãŸæŠ½å‡º
        Args:
            text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
            extraction_type: æŠ½å‡ºã‚¿ã‚¤ãƒ— (character, emotion, relationship)
            extraction_passes: æŠ½å‡ºãƒ‘ã‚¹æ•°
            max_workers: ä¸¦åˆ—å‡¦ç†ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
            max_char_buffer: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º
        Returns:
            æŠ½å‡ºçµæœã®ãƒªã‚¹ãƒˆ
        """
        print(f"  ğŸ“¡ {extraction_type}ã‚’Scalingæ©Ÿèƒ½ã§æŠ½å‡ºä¸­...")
        
        try:
            # LangExtractã®Scalingæ©Ÿèƒ½ã‚’ä½¿ç”¨
            result = lx.extract(
                text_or_documents=text,
                prompt_description=self.prompts[extraction_type],
                examples=self.examples[extraction_type],
                model_id=self.model_id,
                api_key=self.api_key,
                # Scaling parameters
                extraction_passes=extraction_passes,  # è¤‡æ•°ãƒ‘ã‚¹ã§ç²¾åº¦å‘ä¸Š
                max_workers=max_workers,             # ä¸¦åˆ—å‡¦ç†
                max_char_buffer=max_char_buffer      # å°ã•ãªãƒãƒƒãƒ•ã‚¡ã§ç²¾åº¦å‘ä¸Š
            )
            
            # çµæœã‚’é©åˆ‡ãªãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ã«å¤‰æ›
            if extraction_type == "character":
                items = []
                for extraction in result.extractions:
                    attrs = extraction.attributes
                    char = Character(
                        name=extraction.extraction_text or attrs.get("name", "ä¸æ˜"),
                        gender=attrs.get("gender", "ä¸æ˜"),
                        age=attrs.get("age"),
                        occupation=attrs.get("occupation"),
                        appearance=attrs.get("appearance"),
                        personality=attrs.get("personality")
                    )
                    items.append(char)
            
            elif extraction_type == "emotion":
                items = []
                for extraction in result.extractions:
                    attrs = extraction.attributes
                    emotion = Emotion(
                        emotion_type=attrs.get("emotion_type", "ä¸æ˜"),
                        subject=attrs.get("subject", "ä¸æ˜"),
                        target=attrs.get("target"),
                        intensity=attrs.get("intensity", "æ™®é€š"),
                        quote=extraction.extraction_text or attrs.get("quote", "")
                    )
                    items.append(emotion)
            
            elif extraction_type == "relationship":
                items = []
                for extraction in result.extractions:
                    attrs = extraction.attributes
                    rel = Relationship(
                        person1=attrs.get("person1", "ä¸æ˜"),
                        person2=attrs.get("person2", "ä¸æ˜"),
                        relation_type=attrs.get("relation_type", "ä¸æ˜"),
                        direction=attrs.get("direction", "ä¸€æ–¹å‘"),
                        evidence=extraction.extraction_text or attrs.get("evidence", "")
                    )
                    items.append(rel)
            
            else:
                items = []
            
            print(f"    â†’ Scalingæ©Ÿèƒ½ã§{len(items)}å€‹ã®{extraction_type}ã‚’ç™ºè¦‹")
            return items
            
        except Exception as e:
            print(f"    âŒ Scalingæ©Ÿèƒ½ã§ã‚¨ãƒ©ãƒ¼: {e}")
            raise e
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
    def _get_character_prompt(self) -> str:
        return """
ã‚ãªãŸã¯æ—¥æœ¬æ–‡å­¦ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®å°èª¬ã‹ã‚‰ç™»å ´äººç‰©ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

æŠ½å‡ºã™ã‚‹æƒ…å ±:
- name: äººç‰©åï¼ˆãƒ•ãƒ«ãƒãƒ¼ãƒ ã€æ„›ç§°ã€å‘¼ã³åã™ã¹ã¦ï¼‰
- gender: æ€§åˆ¥ï¼ˆç”·æ€§/å¥³æ€§/ä¸æ˜ï¼‰
- age: å¹´é½¢ï¼ˆæ˜è¨˜ã•ã‚Œã¦ã„ã‚Œã°æ•°å€¤ã€ãªã‘ã‚Œã°æ¨å®šï¼šå­ä¾›/è‹¥è€…/ä¸­å¹´/è€äººï¼‰
- occupation: è·æ¥­ãƒ»èº«åˆ†
- appearance: å¤–è¦‹çš„ç‰¹å¾´
- personality: æ€§æ ¼ãƒ»äººæŸ„ï¼ˆå…·ä½“çš„ãªæå†™ã‹ã‚‰æ¨æ¸¬ï¼‰

æ³¨æ„äº‹é …:
- ã€Œç§ã€ã€Œåƒ•ã€ãªã©ã®ä¸€äººç§°ã‚‚äººç‰©ã¨ã—ã¦æ‰±ã†
- åŒä¸€äººç‰©ã®ç•°ãªã‚‹å‘¼ã³åã¯çµ±åˆã™ã‚‹
- æ¨æ¸¬ã®å ´åˆã¯å¿…ãšã€Œæ¨å®šã€ã¨æ˜è¨˜ã™ã‚‹
"""
    
    def _get_emotion_prompt(self) -> str:
        return """
ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ„Ÿæƒ…è¡¨ç¾ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

æŠ½å‡ºã™ã‚‹æƒ…å ±:
- emotion_type: æ„Ÿæƒ…ã®ç¨®é¡ï¼ˆå–œã³ã€æ‚²ã—ã¿ã€æ€’ã‚Šã€æã‚Œã€é©šãã€å«Œæ‚ªã€æœŸå¾…ã€ä¿¡é ¼ãªã©ï¼‰
- subject: æ„Ÿæƒ…ã®ä¸»ä½“ï¼ˆèª°ã®æ„Ÿæƒ…ã‹ï¼‰
- target: æ„Ÿæƒ…ã®å¯¾è±¡ï¼ˆä½•ã«å¯¾ã™ã‚‹æ„Ÿæƒ…ã‹ï¼‰
- intensity: æ„Ÿæƒ…ã®å¼·åº¦ï¼ˆå¼±ã„/æ™®é€š/å¼·ã„ï¼‰
- quote: åŸæ–‡ã®è©²å½“ç®‡æ‰€ï¼ˆæ­£ç¢ºã«å¼•ç”¨ï¼‰

æ—¥æœ¬èªç‰¹æœ‰ã®è¡¨ç¾ã«æ³¨æ„:
- é–“æ¥çš„ãªæ„Ÿæƒ…è¡¨ç¾ï¼ˆã€Œã€œãã†ã ã€ã€Œã€œã‚‰ã—ã„ã€ï¼‰
- æ“¬æ…‹èªãƒ»æ“¬éŸ³èªã«ã‚ˆã‚‹æ„Ÿæƒ…è¡¨ç¾
- æ–‡æœ«è¡¨ç¾ã«ã‚ˆã‚‹æ„Ÿæƒ…ã®ãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹
"""
    
    def _get_relationship_prompt(self) -> str:
        return """
ç™»å ´äººç‰©é–“ã®é–¢ä¿‚æ€§ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

æŠ½å‡ºã™ã‚‹æƒ…å ±:
- person1: äººç‰©1ã®åå‰
- person2: äººç‰©2ã®åå‰
- relation_type: é–¢ä¿‚ã®ç¨®é¡ï¼ˆå®¶æ—/å‹äºº/æ‹äºº/ä¸Šå¸éƒ¨ä¸‹/å¸«å¼Ÿ/æ•µå¯¾ãªã©ï¼‰
- direction: é–¢ä¿‚ã®æ–¹å‘æ€§ï¼ˆä¸€æ–¹å‘/åŒæ–¹å‘ï¼‰
- evidence: é–¢ä¿‚æ€§ã®æ ¹æ‹ ã¨ãªã‚‹æ–‡ç« ã®å¼•ç”¨

é–¢ä¿‚æ€§ã®ç¨®é¡ã®ä¾‹:
- å®¶æ—é–¢ä¿‚ï¼ˆè¦ªå­ã€å…„å¼Ÿã€å¤«å©¦ãªã©ï¼‰
- ç¤¾ä¼šçš„é–¢ä¿‚ï¼ˆä¸Šå¸éƒ¨ä¸‹ã€å¸«å¼Ÿã€åŒåƒšãªã©ï¼‰
- å€‹äººçš„é–¢ä¿‚ï¼ˆå‹äººã€æ‹äººã€æ•µå¯¾ãªã©ï¼‰
"""
    
    # æŠ½å‡ºä¾‹ã®å®šç¾©
    def _get_character_examples(self) -> List:
        return [
            lx.data.ExampleData(
                text="ç§ã¯çŒ«ã§ã‚ã‚‹ã€‚åå‰ã¯ã¾ã ç„¡ã„ã€‚",
                extractions=[
                    lx.data.Extraction(
                        extraction_class="character",
                        extraction_text="ç§",
                        attributes={
                            "name": "ç§ï¼ˆçŒ«ï¼‰",
                            "gender": "ä¸æ˜",
                            "age": "ä¸æ˜",
                            "occupation": "ãªã—ï¼ˆçŒ«ï¼‰",
                            "appearance": "çŒ«",
                            "personality": "è¦³å¯ŸåŠ›ãŒé‹­ã„ã€å“²å­¦çš„"
                        }
                    )
                ]
            )
        ]
    
    def _get_emotion_examples(self) -> List:
        return [
            lx.data.ExampleData(
                text="ãƒ¡ãƒ­ã‚¹ã¯æ¿€æ€’ã—ãŸã€‚å¿…ãšã€ã‹ã®é‚ªæ™ºæš´è™ã®ç‹ã‚’é™¤ã‹ãªã‘ã‚Œã°ãªã‚‰ã¬ã¨æ±ºæ„ã—ãŸã€‚",
                extractions=[
                    lx.data.Extraction(
                        extraction_class="emotion",
                        extraction_text="ãƒ¡ãƒ­ã‚¹ã¯æ¿€æ€’ã—ãŸ",
                        attributes={
                            "emotion_type": "æ€’ã‚Š",
                            "subject": "ãƒ¡ãƒ­ã‚¹",
                            "target": "ç‹",
                            "intensity": "å¼·ã„"
                        }
                    )
                ]
            )
        ]
    
    def _get_relationship_examples(self) -> List:
        return [
            lx.data.ExampleData(
                text="ãƒ¡ãƒ­ã‚¹ã«ã¯å¦¹ãŒã„ã‚‹ã€‚åå…­æ­³ã§ã€æ‘ã®ç‰§äººã¨å©šç´„ã—ã¦ã„ãŸã€‚",
                extractions=[
                    lx.data.Extraction(
                        extraction_class="relationship",
                        extraction_text="ãƒ¡ãƒ­ã‚¹ã«ã¯å¦¹ãŒã„ã‚‹",
                        attributes={
                            "person1": "ãƒ¡ãƒ­ã‚¹",
                            "person2": "å¦¹",
                            "relation_type": "å…„å¦¹",
                            "direction": "åŒæ–¹å‘"
                        }
                    )
                ]
            )
        ]