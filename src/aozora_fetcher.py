"""
é’ç©ºæ–‡åº«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ãƒ»å‰å‡¦ç†ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
"""
import re
import requests
from bs4 import BeautifulSoup


class AozoraFetcher:
    """é’ç©ºæ–‡åº«ã®ãƒ†ã‚­ã‚¹ãƒˆå–å¾—ã‚¯ãƒ©ã‚¹"""
    
    # ã‚µãƒ³ãƒ—ãƒ«ä½œå“ãƒªã‚¹ãƒˆ
    SAMPLE_WORKS = {
        "ç¾…ç”Ÿé–€": {
            "author": "èŠ¥å·é¾ä¹‹ä»‹",
            "url": "https://www.aozora.gr.jp/cards/000879/files/127_15260.html"
        },
        "åŠã£ã¡ã‚ƒã‚“": {
            "author": "å¤ç›®æ¼±çŸ³", 
            "url": "https://www.aozora.gr.jp/cards/000148/files/752_14964.html"
        },
        "èµ°ã‚Œãƒ¡ãƒ­ã‚¹": {
            "author": "å¤ªå®°æ²»",
            "url": "https://www.aozora.gr.jp/cards/000035/files/1567_14913.html"
        },
        "éŠ€æ²³é‰„é“ã®å¤œ": {
            "author": "å®®æ²¢è³¢æ²»",
            "url": "https://www.aozora.gr.jp/cards/000081/files/456_15050.html"
        }
    }
    
    def fetch_sample(self, work_name: str) -> str:
        """ã‚µãƒ³ãƒ—ãƒ«ä½œå“ã‚’å–å¾—"""
        if work_name not in self.SAMPLE_WORKS:
            raise ValueError(f"ä½œå“åãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {work_name}")
        
        work_info = self.SAMPLE_WORKS[work_name]
        print(f"ğŸ“š {work_name}ï¼ˆ{work_info['author']}ï¼‰ã‚’å–å¾—ä¸­...")
        
        text = self.fetch_from_url(work_info['url'])
        normalized_text = self.normalize_text(text)
        
        print(f"âœ… å–å¾—å®Œäº†: {len(normalized_text)}æ–‡å­—")
        return normalized_text
    
    def fetch_from_url(self, url: str) -> str:
        """æŒ‡å®šURLã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—"""
        try:
            response = requests.get(url, timeout=30)
            response.encoding = 'shift_jis'  # é’ç©ºæ–‡åº«ã®æ–‡å­—ã‚³ãƒ¼ãƒ‰
            
            # HTMLã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†ã‚’æŠ½å‡º
            soup = BeautifulSoup(response.content, 'html.parser', from_encoding='shift_jis')
            
            # æœ¬æ–‡ã‚’å«ã‚€divã‚’æ¢ã™
            main_text = soup.find('div', class_='main_text')
            if not main_text:
                # å¤ã„å½¢å¼ã®å ´åˆã€bodyã‹ã‚‰ç›´æ¥å–å¾—
                main_text = soup.body
                if not main_text:
                    # ã•ã‚‰ã«å¤ã„å½¢å¼ã®å ´åˆã€å…¨ä½“ã‹ã‚‰å–å¾—
                    main_text = soup
            
            # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
            text = main_text.get_text()
            
            return text
            
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            print("ğŸ“ ãƒ†ã‚¹ãƒˆç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return ""
    
    def normalize_text(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã®æ­£è¦åŒ–ï¼ˆãƒ«ãƒ“ãƒ»æ³¨é‡ˆã®é™¤å»ï¼‰"""
        normalizer = TextNormalizer()
        return normalizer.normalize(text)


class TextNormalizer:
    """é’ç©ºæ–‡åº«å½¢å¼ã®ãƒ†ã‚­ã‚¹ãƒˆæ­£è¦åŒ–ã‚¯ãƒ©ã‚¹"""
    
    # ãƒ«ãƒ“ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
    RUBY_PATTERNS = [
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: ï½œæ¼¢å­—ã€Šã‹ã‚“ã˜ã€‹
        (r'ï½œ([^ã€Š]+)ã€Š[^ã€‹]+ã€‹', r'\1'),
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: æ¼¢å­—ã€Šã‹ã‚“ã˜ã€‹ï¼ˆï½œãªã—ï¼‰
        (r'([ä¸€-é¾¥ã€…ã-ã‚”ã‚¡-ãƒ´ãƒ¼]+)ã€Š[^ã€‹]+ã€‹', r'\1'),
        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: ï¼»ï¼ƒãƒ«ãƒ“é–¢é€£ï¼½
        (r'ï¼»ï¼ƒ[^ï¼½]*ãƒ«ãƒ“[^ï¼½]*ï¼½', ''),
    ]
    
    # æ³¨é‡ˆã®ãƒ‘ã‚¿ãƒ¼ãƒ³
    ANNOTATION_PATTERNS = [
        # ï¼»ï¼ƒ...ï¼½å½¢å¼ã®æ³¨é‡ˆ
        (r'ï¼»ï¼ƒ[^ï¼½]+ï¼½', ''),
        # â€»ï¼»ï¼ƒ...ï¼½å½¢å¼
        (r'â€»ï¼»ï¼ƒ[^ï¼½]+ï¼½', ''),
    ]
    
    def normalize(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’æ­£è¦åŒ–"""
        # 1. ãƒ«ãƒ“ã®é™¤å»
        for pattern, replacement in self.RUBY_PATTERNS:
            text = re.sub(pattern, replacement, text)
        
        # 2. æ³¨é‡ˆã®é™¤å»
        for pattern, replacement in self.ANNOTATION_PATTERNS:
            text = re.sub(pattern, replacement, text)
        
        # 3. æ”¹è¡Œãƒ»ç©ºç™½ã®æ­£è¦åŒ–
        text = re.sub(r'\r\n', '\n', text)  # æ”¹è¡Œã‚³ãƒ¼ãƒ‰çµ±ä¸€
        text = re.sub(r'ã€€+', 'ã€€', text)   # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹é‡è¤‡é™¤å»
        text = re.sub(r'\n\n\n+', '\n\n', text)  # éå‰°ãªç©ºè¡Œã‚’å‰Šé™¤
        text = re.sub(r' +', ' ', text)     # åŠè§’ã‚¹ãƒšãƒ¼ã‚¹é‡è¤‡é™¤å»
        
        # 4. ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ»ãƒ•ãƒƒã‚¿ãƒ¼ã®é™¤å»
        text = self._remove_headers_footers(text)
        
        return text.strip()
    
    def _remove_headers_footers(self, text: str) -> str:
        """ä½œå“æœ¬æ–‡ä»¥å¤–ã®éƒ¨åˆ†ã‚’é™¤å»"""
        lines = text.split('\n')
        
        # æœ¬æ–‡é–‹å§‹ä½ç½®ã‚’æ¢ã™
        start_idx = 0
        for i, line in enumerate(lines):
            # ä¸€èˆ¬çš„ãªæœ¬æ–‡é–‹å§‹ãƒ‘ã‚¿ãƒ¼ãƒ³
            if any(marker in line for marker in ['-------', 'ã€ãƒ†ã‚­ã‚¹ãƒˆä¸­ã«ç¾ã‚Œã‚‹è¨˜å·ã«ã¤ã„ã¦ã€‘']):
                start_idx = i + 1
                break
        
        # æœ¬æ–‡çµ‚äº†ä½ç½®ã‚’æ¢ã™
        end_idx = len(lines)
        for i in range(len(lines) - 1, -1, -1):
            line = lines[i]
            # ä¸€èˆ¬çš„ãªæœ¬æ–‡çµ‚äº†ãƒ‘ã‚¿ãƒ¼ãƒ³
            if any(marker in line for marker in ['åº•æœ¬ï¼š', 'å…¥åŠ›ï¼š', 'æ ¡æ­£ï¼š', 'â€»ï¼»ï¼ƒ']):
                end_idx = i
                break
        
        # æœ¬æ–‡éƒ¨åˆ†ã®ã¿ã‚’è¿”ã™
        return '\n'.join(lines[start_idx:end_idx])


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    fetcher = AozoraFetcher()
    
    # ç¾…ç”Ÿé–€ã‚’å–å¾—ã—ã¦ã¿ã‚‹
    text = fetcher.fetch_sample("ç¾…ç”Ÿé–€")
    
    # æœ€åˆã®500æ–‡å­—ã‚’è¡¨ç¤º
    print("\nğŸ“„ å–å¾—ã—ãŸãƒ†ã‚­ã‚¹ãƒˆï¼ˆæœ€åˆã®500æ–‡å­—ï¼‰:")
    print("-" * 50)
    print(text[:500])
    print("-" * 50)