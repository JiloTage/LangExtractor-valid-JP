# è©³ç´°è¨­è¨ˆæ›¸: æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆè§£æã‚·ã‚¹ãƒ†ãƒ 

## 1. é’ç©ºæ–‡åº«ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹è©³ç´°è¨­è¨ˆ

### 1.1 ãƒ†ã‚­ã‚¹ãƒˆå–å¾—æ–¹æ³•

#### æ–¹æ³•1: é’ç©ºæ–‡åº«API (æ¨å¥¨)
```python
import requests
from typing import Optional, Dict
import re

class AozoraAPI:
    BASE_URL = "https://www.aozora.gr.jp/cards"
    
    def get_text_by_id(self, author_id: str, work_id: str) -> str:
        """
        ä½œå“IDã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
        ä¾‹: å¤ç›®æ¼±çŸ³(000148)ã®åŠã£ã¡ã‚ƒã‚“(000752)
        URL: https://www.aozora.gr.jp/cards/000148/files/752_ruby_2438.html
        """
        # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®URLæ§‹ç¯‰
        url = f"{self.BASE_URL}/{author_id}/files/{work_id}_ruby_*.html"
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‹ã‚‰é©åˆ‡ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ
        
    def get_text_from_github(self, work_id: str) -> str:
        """
        é’ç©ºæ–‡åº«ã®GitHubãƒŸãƒ©ãƒ¼ã‹ã‚‰å–å¾—ï¼ˆã‚ˆã‚Šå®‰å®šï¼‰
        https://github.com/aozorabunko/aozorabunko
        """
        pass
```

#### æ–¹æ³•2: ç›´æ¥URLæŒ‡å®š
```python
def fetch_from_url(self, url: str) -> str:
    """ä»»æ„ã®URLã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆå–å¾—"""
    response = requests.get(url)
    response.encoding = 'shift_jis'  # é’ç©ºæ–‡åº«ã®æ–‡å­—ã‚³ãƒ¼ãƒ‰
    return response.text
```

### 1.2 ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ã®è©³ç´°

#### ãƒ«ãƒ“å‡¦ç†ãƒ‘ã‚¿ãƒ¼ãƒ³
```python
class TextNormalizer:
    # ãƒ«ãƒ“ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
    RUBY_PATTERNS = [
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: ï½œæ¼¢å­—ã€Šã‹ã‚“ã˜ã€‹
        (r'ï½œ([^ã€Š]+)ã€Š[^ã€‹]+ã€‹', r'\1'),
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: æ¼¢å­—ã€Šã‹ã‚“ã˜ã€‹ï¼ˆï½œãªã—ï¼‰
        (r'([ä¸€-é¾¥ã€…]+)ã€Š[^ã€‹]+ã€‹', r'\1'),
        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: ï½œæ¼¢å­—ï¼ˆã‹ã‚“ã˜ï¼‰
        (r'ï½œ([^ï¼ˆ]+)ï¼ˆ[^ï¼‰]+ï¼‰', r'\1'),
    ]
    
    # æ³¨é‡ˆã®ãƒ‘ã‚¿ãƒ¼ãƒ³
    ANNOTATION_PATTERNS = [
        # ï¼»ï¼ƒã€Œ...ã€ã«å‚ç‚¹ï¼½
        (r'ï¼»ï¼ƒ[^ï¼½]+ï¼½', ''),
        # â€»ï¼»ï¼ƒ...ï¼½
        (r'â€»ï¼»ï¼ƒ[^ï¼½]+ï¼½', ''),
    ]
    
    def normalize(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã®æ­£è¦åŒ–å‡¦ç†"""
        # 1. ãƒ«ãƒ“ã®é™¤å»
        for pattern, replacement in self.RUBY_PATTERNS:
            text = re.sub(pattern, replacement, text)
            
        # 2. æ³¨é‡ˆã®é™¤å»
        for pattern, replacement in self.ANNOTATION_PATTERNS:
            text = re.sub(pattern, replacement, text)
            
        # 3. æ”¹è¡Œãƒ»ç©ºç™½ã®æ­£è¦åŒ–
        text = re.sub(r'\r\n', '\n', text)  # æ”¹è¡Œã‚³ãƒ¼ãƒ‰çµ±ä¸€
        text = re.sub(r'ã€€+', 'ã€€', text)   # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹é‡è¤‡é™¤å»
        text = re.sub(r'\n\n+', '\n\n', text)  # ç©ºè¡Œã®æ­£è¦åŒ–
        
        # 4. é’ç©ºæ–‡åº«ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ»ãƒ•ãƒƒã‚¿ãƒ¼ã®é™¤å»
        text = self._remove_headers_footers(text)
        
        return text
    
    def _remove_headers_footers(self, text: str) -> str:
        """ä½œå“æœ¬æ–‡ä»¥å¤–ã®éƒ¨åˆ†ã‚’é™¤å»"""
        lines = text.split('\n')
        
        # æœ¬æ–‡é–‹å§‹ãƒ»çµ‚äº†ãƒãƒ¼ã‚«ãƒ¼ã‚’æ¢ã™
        start_markers = ['-------', 'ã€ãƒ†ã‚­ã‚¹ãƒˆä¸­ã«ç¾ã‚Œã‚‹è¨˜å·ã«ã¤ã„ã¦ã€‘']
        end_markers = ['åº•æœ¬ï¼š', 'â€»ï¼»ï¼ƒ']
        
        # å®Ÿè£…çœç•¥
        return '\n'.join(lines[start:end])
```

### 1.3 ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆå–å¾—é–¢æ•°
```python
# å‹•ä½œç¢ºèªç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ä½œå“ãƒªã‚¹ãƒˆ
SAMPLE_WORKS = {
    "ç¾…ç”Ÿé–€": {
        "author_id": "000879",  # èŠ¥å·é¾ä¹‹ä»‹
        "work_id": "127",
        "url": "https://www.aozora.gr.jp/cards/000879/files/127_ruby_150.html"
    },
    "åŠã£ã¡ã‚ƒã‚“": {
        "author_id": "000148",  # å¤ç›®æ¼±çŸ³
        "work_id": "752",
        "url": "https://www.aozora.gr.jp/cards/000148/files/752_ruby_2438.html"
    },
    "èµ°ã‚Œãƒ¡ãƒ­ã‚¹": {
        "author_id": "000035",  # å¤ªå®°æ²»
        "work_id": "1567",
        "url": "https://www.aozora.gr.jp/cards/000035/files/1567_ruby_4948.html"
    }
}
```

## 2. LangExtractå®Ÿè¡Œã‚¨ãƒ³ã‚¸ãƒ³è©³ç´°è¨­è¨ˆ

### 2.1 æ—¥æœ¬èªç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°

#### åŸºæœ¬ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹é€ 
```python
class JapanesePromptTemplates:
    
    CHARACTER_EXTRACTION = """
ã‚ãªãŸã¯æ—¥æœ¬æ–‡å­¦ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®å°èª¬ã‹ã‚‰ç™»å ´äººç‰©ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

æŠ½å‡ºã™ã‚‹æƒ…å ±:
1. äººç‰©åï¼ˆãƒ•ãƒ«ãƒãƒ¼ãƒ ã€æ„›ç§°ã€å‘¼ã³åã™ã¹ã¦ï¼‰
2. æ€§åˆ¥ï¼ˆç”·æ€§/å¥³æ€§/ä¸æ˜ï¼‰
3. å¹´é½¢ï¼ˆæ˜è¨˜ã•ã‚Œã¦ã„ã‚Œã°æ•°å€¤ã€ãªã‘ã‚Œã°æ¨å®šï¼šå­ä¾›/è‹¥è€…/ä¸­å¹´/è€äººï¼‰
4. è·æ¥­ãƒ»èº«åˆ†
5. å¤–è¦‹çš„ç‰¹å¾´
6. æ€§æ ¼ãƒ»äººæŸ„ï¼ˆå…·ä½“çš„ãªæå†™ã‹ã‚‰æ¨æ¸¬ï¼‰

æ³¨æ„äº‹é …:
- ã€Œç§ã€ã€Œåƒ•ã€ãªã©ã®ä¸€äººç§°ã‚‚äººç‰©ã¨ã—ã¦æ‰±ã†
- åŒä¸€äººç‰©ã®ç•°ãªã‚‹å‘¼ã³åã¯çµ±åˆã™ã‚‹
- æ¨æ¸¬ã®å ´åˆã¯å¿…ãšã€Œæ¨å®šã€ã¨æ˜è¨˜ã™ã‚‹
"""

    EMOTION_EXTRACTION = """
ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ„Ÿæƒ…è¡¨ç¾ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

æŠ½å‡ºã™ã‚‹æƒ…å ±:
1. æ„Ÿæƒ…ã®ç¨®é¡ï¼ˆå–œã³ã€æ‚²ã—ã¿ã€æ€’ã‚Šã€æã‚Œã€é©šãã€å«Œæ‚ªã€æœŸå¾…ã€ä¿¡é ¼ãªã©ï¼‰
2. æ„Ÿæƒ…ã®ä¸»ä½“ï¼ˆèª°ã®æ„Ÿæƒ…ã‹ï¼‰
3. æ„Ÿæƒ…ã®å¯¾è±¡ï¼ˆä½•ã«å¯¾ã™ã‚‹æ„Ÿæƒ…ã‹ï¼‰
4. æ„Ÿæƒ…ã®å¼·åº¦ï¼ˆå¼±ã„/æ™®é€š/å¼·ã„ï¼‰
5. åŸæ–‡ã®è©²å½“ç®‡æ‰€ï¼ˆæ­£ç¢ºã«å¼•ç”¨ï¼‰

æ—¥æœ¬èªç‰¹æœ‰ã®è¡¨ç¾ã«æ³¨æ„:
- é–“æ¥çš„ãªæ„Ÿæƒ…è¡¨ç¾ï¼ˆã€Œã€œãã†ã ã€ã€Œã€œã‚‰ã—ã„ã€ï¼‰
- æ“¬æ…‹èªãƒ»æ“¬éŸ³èªã«ã‚ˆã‚‹æ„Ÿæƒ…è¡¨ç¾
- æ–‡æœ«è¡¨ç¾ã«ã‚ˆã‚‹æ„Ÿæƒ…ã®ãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹
"""

    RELATIONSHIP_EXTRACTION = """
ç™»å ´äººç‰©é–“ã®é–¢ä¿‚æ€§ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

é–¢ä¿‚æ€§ã®ç¨®é¡:
- å®¶æ—é–¢ä¿‚ï¼ˆè¦ªå­ã€å…„å¼Ÿã€å¤«å©¦ãªã©ï¼‰
- ç¤¾ä¼šçš„é–¢ä¿‚ï¼ˆä¸Šå¸éƒ¨ä¸‹ã€å¸«å¼Ÿã€åŒåƒšãªã©ï¼‰
- å€‹äººçš„é–¢ä¿‚ï¼ˆå‹äººã€æ‹äººã€æ•µå¯¾ãªã©ï¼‰

æŠ½å‡ºå½¢å¼:
- äººç‰©A â†’ é–¢ä¿‚ã®ç¨®é¡ â†’ äººç‰©B
- åŒæ–¹å‘ã®å ´åˆã¯ä¸¡æ–¹è¨˜è¼‰
- é–¢ä¿‚æ€§ã®æ ¹æ‹ ã¨ãªã‚‹æ–‡ç« ã‚’å¼•ç”¨
"""
```

#### å®Ÿè¡Œä¾‹ï¼ˆExampleï¼‰ã®è¨­è¨ˆ
```python
def create_extraction_examples():
    """LangExtractç”¨ã®ä¾‹ã‚’ä½œæˆ"""
    
    character_example = {
        "text": "ç§ã¯çŒ«ã§ã‚ã‚‹ã€‚åå‰ã¯ã¾ã ç„¡ã„ã€‚",
        "extracted": [
            {
                "name": "ç§ï¼ˆçŒ«ï¼‰",
                "gender": "ä¸æ˜",
                "age": "ä¸æ˜",
                "occupation": "ãªã—ï¼ˆçŒ«ï¼‰",
                "appearance": "çŒ«",
                "personality": "è¦³å¯ŸåŠ›ãŒé‹­ã„ã€å“²å­¦çš„"
            }
        ]
    }
    
    emotion_example = {
        "text": "ãƒ¡ãƒ­ã‚¹ã¯æ¿€æ€’ã—ãŸã€‚å¿…ãšã€ã‹ã®é‚ªæ™ºæš´è™ã®ç‹ã‚’é™¤ã‹ãªã‘ã‚Œã°ãªã‚‰ã¬ã¨æ±ºæ„ã—ãŸã€‚",
        "extracted": [
            {
                "emotion": "æ€’ã‚Š",
                "subject": "ãƒ¡ãƒ­ã‚¹",
                "target": "ç‹",
                "intensity": "å¼·ã„",
                "quote": "ãƒ¡ãƒ­ã‚¹ã¯æ¿€æ€’ã—ãŸ"
            }
        ]
    }
    
    return [character_example], [emotion_example]
```

### 2.2 æŠ½å‡ºã‚¹ã‚­ãƒ¼ãƒã®è©³ç´°å®šç¾©

```python
from dataclasses import dataclass
from typing import List, Optional
from enum import Enum

class Gender(Enum):
    MALE = "ç”·æ€§"
    FEMALE = "å¥³æ€§"
    UNKNOWN = "ä¸æ˜"

class EmotionType(Enum):
    JOY = "å–œã³"
    SADNESS = "æ‚²ã—ã¿"
    ANGER = "æ€’ã‚Š"
    FEAR = "æã‚Œ"
    SURPRISE = "é©šã"
    DISGUST = "å«Œæ‚ª"
    TRUST = "ä¿¡é ¼"
    ANTICIPATION = "æœŸå¾…"

@dataclass
class Character:
    name: str
    aliases: List[str]  # åˆ¥åãƒ»æ„›ç§°
    gender: Gender
    age: Optional[str]
    occupation: Optional[str]
    appearance: Optional[str]
    personality: Optional[str]
    first_appearance: int  # åˆç™»å ´ã®æ–‡å­—ä½ç½®

@dataclass
class Emotion:
    type: EmotionType
    subject: str
    target: Optional[str]
    intensity: str  # å¼±ã„/æ™®é€š/å¼·ã„
    quote: str
    position: int  # ãƒ†ã‚­ã‚¹ãƒˆå†…ã®ä½ç½®

@dataclass
class Relationship:
    person1: str
    person2: str
    relation_type: str
    direction: str  # ä¸€æ–¹å‘/åŒæ–¹å‘
    evidence: str  # æ ¹æ‹ ã¨ãªã‚‹å¼•ç”¨
```

### 2.3 ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²æˆ¦ç•¥

```python
class TextChunker:
    def __init__(self, max_chars: int = 3000, overlap: int = 500):
        """
        max_chars: ãƒãƒ£ãƒ³ã‚¯ã®æœ€å¤§æ–‡å­—æ•°
        overlap: ãƒãƒ£ãƒ³ã‚¯é–“ã®é‡è¤‡æ–‡å­—æ•°
        """
        self.max_chars = max_chars
        self.overlap = overlap
    
    def split_by_scenes(self, text: str) -> List[str]:
        """ã‚·ãƒ¼ãƒ³ï¼ˆæ®µè½ï¼‰å˜ä½ã§åˆ†å‰²"""
        # 1. æ®µè½ã§åˆ†å‰²
        paragraphs = text.split('\n\n')
        
        chunks = []
        current_chunk = ""
        
        for para in paragraphs:
            if len(current_chunk) + len(para) < self.max_chars:
                current_chunk += para + "\n\n"
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = para + "\n\n"
        
        if current_chunk:
            chunks.append(current_chunk)
            
        return chunks
    
    def split_with_context(self, text: str) -> List[Dict[str, any]]:
        """æ–‡è„ˆã‚’ä¿æŒã—ãŸåˆ†å‰²"""
        chunks = self.split_by_scenes(text)
        
        # å„ãƒãƒ£ãƒ³ã‚¯ã«æ–‡è„ˆæƒ…å ±ã‚’ä»˜åŠ 
        chunk_data = []
        for i, chunk in enumerate(chunks):
            context = {
                "chunk_id": i,
                "text": chunk,
                "previous_summary": self._summarize(chunks[i-1]) if i > 0 else None,
                "position": sum(len(c) for c in chunks[:i])
            }
            chunk_data.append(context)
            
        return chunk_data
```

## 3. çµæœåˆ†æã‚·ã‚¹ãƒ†ãƒ è©³ç´°è¨­è¨ˆ

### 3.1 è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å®šç¾©

```python
class ExtractionMetrics:
    """æŠ½å‡ºçµæœã®è©•ä¾¡æŒ‡æ¨™"""
    
    def calculate_extraction_stats(self, results):
        return {
            "total_characters": len(results.characters),
            "unique_characters": len(set(c.name for c in results.characters)),
            "emotions_per_character": self._emotions_per_character(results),
            "relationship_density": len(results.relationships) / len(results.characters),
            "extraction_coverage": self._calculate_coverage(results)
        }
    
    def analyze_japanese_specific(self, results):
        """æ—¥æœ¬èªç‰¹æœ‰ã®åˆ†æ"""
        return {
            "honorific_usage": self._analyze_honorifics(results),
            "emotion_indirectness": self._analyze_indirect_emotions(results),
            "name_variations": self._analyze_name_variations(results)
        }
```

### 3.2 å¯è¦–åŒ–ã®å…·ä½“çš„ãªUI/UX

#### 3.2.1 HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
```python
class ResultVisualizer:
    def generate_html_report(self, results, original_text):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        
        html_template = """
        <!DOCTYPE html>
        <html>
        <head>
            <meta charset="utf-8">
            <title>LangExtract æ—¥æœ¬èªè§£æçµæœ</title>
            <style>
                .character { background-color: #e3f2fd; padding: 2px; cursor: pointer; }
                .emotion { background-color: #fff3e0; padding: 2px; }
                .relationship { text-decoration: underline; }
                .sidebar { position: fixed; right: 0; width: 300px; 
                          height: 100%; overflow-y: scroll; }
            </style>
        </head>
        <body>
            <div class="main-content">
                {annotated_text}
            </div>
            <div class="sidebar">
                <h2>æŠ½å‡ºçµæœã‚µãƒãƒªãƒ¼</h2>
                {summary}
            </div>
        </body>
        </html>
        """
        
        annotated = self._annotate_text(original_text, results)
        summary = self._generate_summary(results)
        
        return html_template.format(
            annotated_text=annotated,
            summary=summary
        )
```

#### 3.2.2 ã‚¿ãƒ¼ãƒŸãƒŠãƒ«è¡¨ç¤ºç”¨
```python
def display_results_cli(results):
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ç”¨ã®çµæœè¡¨ç¤º"""
    
    print("="*50)
    print("ğŸ“š LangExtract è§£æçµæœ")
    print("="*50)
    
    print("\nğŸ‘¤ ç™»å ´äººç‰©:")
    for char in results.characters:
        print(f"  â€¢ {char.name} ({char.gender.value}, {char.age or 'å¹´é½¢ä¸æ˜'})")
        if char.personality:
            print(f"    æ€§æ ¼: {char.personality}")
    
    print("\nğŸ’­ æ„Ÿæƒ…åˆ†æ:")
    emotion_counts = {}
    for emotion in results.emotions:
        emotion_counts[emotion.type] = emotion_counts.get(emotion.type, 0) + 1
    
    for emotion_type, count in emotion_counts.items():
        print(f"  â€¢ {emotion_type}: {count}å›")
    
    print("\nğŸ”— é–¢ä¿‚æ€§:")
    for rel in results.relationships:
        print(f"  â€¢ {rel.person1} â† {rel.relation_type} â†’ {rel.person2}")
```

### 3.3 æ¯”è¼ƒåˆ†ææ©Ÿèƒ½

```python
class ComparativeAnalyzer:
    """è¤‡æ•°ã®å®Ÿè¡Œçµæœã‚’æ¯”è¼ƒ"""
    
    def compare_models(self, text, models=["gemini-2.0-flash-exp", "gpt-4"]):
        """ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã§ã®çµæœæ¯”è¼ƒ"""
        results = {}
        for model in models:
            results[model] = self._run_extraction(text, model)
        
        comparison = {
            "character_overlap": self._calculate_overlap(results, "characters"),
            "emotion_consistency": self._compare_emotions(results),
            "performance": self._compare_performance(results)
        }
        
        return comparison
    
    def analyze_chunk_consistency(self, chunks_results):
        """ãƒãƒ£ãƒ³ã‚¯é–“ã§ã®æŠ½å‡ºã®ä¸€è²«æ€§ã‚’åˆ†æ"""
        # åŒä¸€äººç‰©ãŒç•°ãªã‚‹åå‰ã§æŠ½å‡ºã•ã‚Œã¦ã„ãªã„ã‹
        # é–¢ä¿‚æ€§ã®çŸ›ç›¾ãŒãªã„ã‹
        # æ„Ÿæƒ…ã®é€£ç¶šæ€§ãŒã‚ã‚‹ã‹
        pass
```

### 3.4 ã‚¯ã‚¤ãƒƒã‚¯å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```python
# quick_test.py
import langextract as lx
from src.aozora_fetcher import AozoraFetcher
from src.text_extractor import JapaneseTextExtractor
from src.result_analyzer import ResultVisualizer

def quick_analyze(work_name="ç¾…ç”Ÿé–€"):
    """ç°¡å˜ã«å®Ÿè¡Œã§ãã‚‹ãƒ†ã‚¹ãƒˆé–¢æ•°"""
    
    # 1. ãƒ†ã‚­ã‚¹ãƒˆå–å¾—
    fetcher = AozoraFetcher()
    text = fetcher.fetch_sample(work_name)
    
    # 2. LangExtractå®Ÿè¡Œ
    extractor = JapaneseTextExtractor()
    results = extractor.extract_all(text)
    
    # 3. çµæœè¡¨ç¤º
    visualizer = ResultVisualizer()
    visualizer.display_results_cli(results)
    
    # 4. HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    html = visualizer.generate_html_report(results, text)
    with open(f"output_{work_name}.html", "w", encoding="utf-8") as f:
        f.write(html)
    
    print(f"\nğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ: output_{work_name}.html")

if __name__ == "__main__":
    quick_analyze()
```